{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e79ff33-fb06-471d-ad74-5c39eb9684be",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">COMP8420 ADV NLP FINAL PROJECT</h2>\n",
    "<h2 align=\"center\">MultiLingAI: Multilingual Contextual Summarization for Global Enterprises</h2>\n",
    "\n",
    "<h2 align=\"center\">Submitted by:<h3>\n",
    "<h4 align=\"center\">Muhammad Haris Rizwan | Student ID: 47565284 </h4>\n",
    "<h4 align=\"center\">Syed Rafay Ali | Student ID: 47833920 </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3598fb6-c384-4599-9edb-cbf3813bc74b",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Dataset](#2.-Dataset)\n",
    "3. [Data Preprocessing](#3.-Data-Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a020fe-6cff-44e4-a134-4fabff121b3d",
   "metadata": {},
   "source": [
    "# __1. Introduction__\n",
    "\n",
    "![MULTILINGAI](images/MULTILINGAI_PIC.jpeg)\n",
    "\n",
    "In this project, we assume the role of engineers at `MultiLinguaAI`, an IT company specializing in advanced Natural Language Processing (NLP) solutions for global enterprises. `MultiLinguaAI` offers a variety of services, including sentiment analysis, text summarization, named entity recognition, and chatbots. Our primary task is to develop and implement a multilingual summarization tool that addresses the unique challenges faced by these enterprises.\n",
    "\n",
    "## __Problem Statement__\n",
    "Global enterprises operate across multiple regions and languages, requiring accurate and context-preserving summaries of documents in various languages. This need is driven by the necessity to streamline operations, enhance communication, and ensure that vital information is accessible and understandable to all stakeholders, regardless of their linguistic background.\n",
    "\n",
    "## __Objective__\n",
    "The objective of our project is to develop a multilingual summarization tool that can generate accurate and contextually relevant summaries for documents written in multiple languages. This tool aims to maintain the integrity and key information of the original documents while making them concise and easy to understand for a diverse global audience.\n",
    "\n",
    "## __Project Scope__\n",
    "The scope of our project involves addressing the real-world challenge of handling and summarizing large volumes of multilingual documents.\n",
    "* Our target users are global enterprises with diverse linguistic documentation needs. \n",
    "* By leveraging advanced NLP models such as mBERT, XLM-R, and multilingual T5, we aim to create a robust solution that can be seamlessly integrated into the company's existing systems.\n",
    "* The project will include data collection, preprocessing, model training, evaluation, and integration phases, ensuring a comprehensive approach to solving this complex problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bb220-a401-452c-b868-44c034114ee8",
   "metadata": {},
   "source": [
    "# __2. Dataset__\n",
    "\n",
    "![dataset](images/dataset_pic.webp)\n",
    "\n",
    "For our project on Multilingual Contextual Summarization for Global Enterprises, the dataset plays a critical role in ensuring the accuracy and relevance of the generated summaries. We have selected datasets that provide a diverse and comprehensive collection of multilingual documents, which are essential for training and evaluating our models.\n",
    "\n",
    "## __Selected Dataset__\n",
    "We will utilize the MLSUM dataset, which stands out as a large-scale multilingual summarization dataset. MLSUM contains over 1.5 million article-summary pairs in five different languages: French, German, Spanish, Russian, and Turkish. This dataset is particularly suitable for our project because it offers a wide variety of articles and summaries from reputable news sources, ensuring both the quality and diversity needed for robust model training.\n",
    "\n",
    "## __References:__\n",
    "* `MLSUM`: The Multilingual Summarization Corpus - This dataset was introduced to facilitate research in multilingual text summarization by providing a large-scale, diverse set of news articles and summaries. It includes articles from five languages and aims to enable new research directions in the text summarization community. Link to paper​​.\n",
    "\n",
    "* `XL-Sum`: Large-Scale Multilingual Abstractive Summarization - XL-Sum provides an extensive collection of multilingual summarization data, enhancing the ability to develop models that perform well across various languages. This dataset complements MLSUM by offering additional resources and benchmarks for evaluating summarization models. Link to paper​​.\n",
    "\n",
    "* Contrastive Aligned Joint Learning for Multilingual Summarization - This reference explores novel methods for improving multilingual summarization, focusing on contrastive learning strategies. It provides insights into the challenges and solutions for developing high-quality summarization models, which will be valuable for refining our approach. Link to paper​​.\n",
    "\n",
    "## __Selected Dataset Details__\n",
    "`MLSUM`: Contains over `1.5 million` article-summary pairs from five languages`.\n",
    "\n",
    "* __Languages__: French, German, Spanish, Russian, Turkish.\n",
    "* __Source__: News articles from reputable sources.\n",
    "* __Data Collection Process__: We will collect the dataset from public repositories and ensure it is preprocessed for tokenization, normalization, and language detection. This preprocessing step is crucial for preparing the data for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9761f76-b09b-476f-874f-55299a4dfb61",
   "metadata": {},
   "source": [
    "# __3. Data Preprocessing__\n",
    "\n",
    "![Process flow](images/process_flow_pic.webp)\n",
    "* __Data cleaning__: Removing unnecessary observations for the sake of project scope.\n",
    "* __Tokenization__: Splitting text into words or subwords to facilitate model understanding.\n",
    "* __Normalization__: Standardizing text data to remove inconsistencies.\n",
    "* __Language Detection__: Identifying and labeling the language of each document to ensure accurate processing.\n",
    "By leveraging the MLSUM dataset and incorporating insights from the referenced works, we aim to develop a robust multilingual summarization tool that meets the needs of global enterprises, providing accurate and context-preserving summaries across multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b54c34ab-ead9-4374-9bc7-3e40b213eaf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "#Load MLSUM dataset for French\n",
    "dataset = load_dataset(\"mlsum\", \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d865cf3-a38b-4545-b463-fe216257b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer\n",
    "tokenizer = MBart50Tokenizer.from_pretrained('facebook/mbart-large-50')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f178b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['text'], max_length=512, padding='max_length', truncation=True)\n",
    "    targets = tokenizer(examples['summary'], max_length=150, padding='max_length', truncation=True)\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': targets['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101574f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51b69dde6724ae5922cdc44245f5ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/392902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying the preprocessing to the dataset\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: Save the processed dataset\n",
    "dataset.save_to_disk(\"processed_mlsum\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
