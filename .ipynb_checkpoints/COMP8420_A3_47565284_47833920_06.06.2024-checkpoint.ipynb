{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e79ff33-fb06-471d-ad74-5c39eb9684be",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">COMP8420 ADV NLP FINAL PROJECT</h2>\n",
    "<h2 align=\"center\">MultiLingAI: Multilingual Contextual Summarization for Global Enterprises</h2>\n",
    "\n",
    "<h2 align=\"center\">Submitted by:<h3>\n",
    "<h4 align=\"center\">Muhammad Haris Rizwan | Student ID: 47565284 </h4>\n",
    "<h4 align=\"center\">Syed Rafay Ali | Student ID: 47833920 </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3598fb6-c384-4599-9edb-cbf3813bc74b",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Dataset](#2.-Dataset)\n",
    "3. [Data Preprocessing](#3.-Data-Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a020fe-6cff-44e4-a134-4fabff121b3d",
   "metadata": {},
   "source": [
    "# __1. Introduction__\n",
    "\n",
    "![MULTILINGAI](images/MULTILINGAI_PIC.jpeg)\n",
    "\n",
    "In this project, we assume the role of engineers at `MultiLinguaAI`, an IT company specializing in advanced Natural Language Processing (NLP) solutions for global enterprises. `MultiLinguaAI` offers a variety of services, including sentiment analysis, text summarization, named entity recognition, and chatbots. Our primary task is to develop and implement a multilingual summarization tool that addresses the unique challenges faced by these enterprises.\n",
    "\n",
    "## __Problem Statement__\n",
    "Global enterprises operate across multiple regions and languages, requiring accurate and context-preserving summaries of documents in various languages. This need is driven by the necessity to streamline operations, enhance communication, and ensure that vital information is accessible and understandable to all stakeholders, regardless of their linguistic background.\n",
    "\n",
    "## __Objective__\n",
    "The objective of our project is to develop a multilingual summarization tool that can generate accurate and contextually relevant summaries for documents written in multiple languages. This tool aims to maintain the integrity and key information of the original documents while making them concise and easy to understand for a diverse global audience.\n",
    "\n",
    "## __Project Scope__\n",
    "The scope of our project involves addressing the real-world challenge of handling and summarizing large volumes of multilingual documents.\n",
    "* Our target users are global enterprises with diverse linguistic documentation needs. \n",
    "* By leveraging advanced NLP models such as mBERT, XLM-R, and multilingual T5, we aim to create a robust solution that can be seamlessly integrated into the company's existing systems.\n",
    "* The project will include data collection, preprocessing, model training, evaluation, and integration phases, ensuring a comprehensive approach to solving this complex problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bb220-a401-452c-b868-44c034114ee8",
   "metadata": {},
   "source": [
    "# __2. Dataset__\n",
    "\n",
    "![dataset](images/dataset_pic.webp)\n",
    "\n",
    "For our project on Multilingual Contextual Summarization for Global Enterprises, the dataset plays a critical role in ensuring the accuracy and relevance of the generated summaries. We have selected datasets that provide a diverse and comprehensive collection of multilingual documents, which are essential for training and evaluating our models.\n",
    "\n",
    "## __Selected Dataset__\n",
    "We will utilize the MLSUM dataset, which stands out as a large-scale multilingual summarization dataset. MLSUM contains over 1.5 million article-summary pairs in five different languages: French, German, Spanish, Russian, and Turkish. This dataset is particularly suitable for our project because it offers a wide variety of articles and summaries from reputable news sources, ensuring both the quality and diversity needed for robust model training.\n",
    "\n",
    "## __References:__\n",
    "* `MLSUM`: The Multilingual Summarization Corpus - This dataset was introduced to facilitate research in multilingual text summarization by providing a large-scale, diverse set of news articles and summaries. It includes articles from five languages and aims to enable new research directions in the text summarization community. Link to paper​​.\n",
    "\n",
    "* `XL-Sum`: Large-Scale Multilingual Abstractive Summarization - XL-Sum provides an extensive collection of multilingual summarization data, enhancing the ability to develop models that perform well across various languages. This dataset complements MLSUM by offering additional resources and benchmarks for evaluating summarization models. Link to paper​​.\n",
    "\n",
    "* Contrastive Aligned Joint Learning for Multilingual Summarization - This reference explores novel methods for improving multilingual summarization, focusing on contrastive learning strategies. It provides insights into the challenges and solutions for developing high-quality summarization models, which will be valuable for refining our approach. Link to paper​​.\n",
    "\n",
    "## __Selected Dataset Details__\n",
    "\n",
    "### __MLSUM:__\n",
    "\n",
    "* Contents: Contains over 1.5 million article-summary pairs from five languages.\n",
    "* Languages: French, German, Spanish, Russian, Turkish.\n",
    "* Source: News articles from reputable sources.\n",
    "* Data Collection Process: We will collect the dataset from public repositories and ensure it is preprocessed for tokenization, normalization, and language detection. This preprocessing step is crucial for preparing the data for model training.\n",
    "### __CNN/DailyMail:__\n",
    "\n",
    "* Contents: Contains over 300,000 article-summary pairs.\n",
    "* Languages: English.\n",
    "* Source: News articles primarily from CNN and the Daily Mail, providing a rich source of diverse topics and high-quality journalism.\n",
    "* Data Collection Process: The dataset is available through Hugging Face and will be directly accessed using the datasets library. It includes preprocessing steps such as tokenization and normalization. The dataset is structured into three splits: train, validation, and test, facilitating the training and evaluation of summarization models. The article column contains the full text, while the highlights column contains the summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9761f76-b09b-476f-874f-55299a4dfb61",
   "metadata": {},
   "source": [
    "# __3. Data Preprocessing__\n",
    "\n",
    "![Process flow](images/process_flow_pic.webp)\n",
    "* __Data Visualisation__: Loading and opening the dataset(s) to see what is in it and what can be done with it.\n",
    "* __Data cleaning__: Removing unnecessary observations for the sake of project scope e.g. Removing extra columns, punctuations, and lowercase the text.\n",
    "* __Normalization__: Standardizing text data to remove inconsistencies.\n",
    "* __Tokenization__: Splitting text into words or subwords to facilitate model understanding.\n",
    "* __Language Detection__: Identifying and labeling the language of each document to ensure accurate processing.\n",
    "By leveraging the MLSUM dataset and incorporating insights from the referenced works, we aim to develop a robust multilingual summarization tool that meets the needs of global enterprises, providing accurate and context-preserving summaries across multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54c34ab-ead9-4374-9bc7-3e40b213eaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harismir/anaconda3/envs/ML/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/harismir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/harismir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relevant libraries\n",
    "\n",
    "# dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import MBart50Tokenizer, MBartForConditionalGeneration\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90ad2b-0690-4bb1-95ee-898338be8188",
   "metadata": {},
   "source": [
    "## STEP 1: Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891b0471-c530-40ae-b11d-3c985aa5269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harismir/anaconda3/envs/ML/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for mlsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mlsum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Load MLSUM dataset for French\n",
    "dataset_fr = load_dataset(\"mlsum\", \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6163f98f-b688-4ab2-9a0a-969d3f74748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MLSUM dataset for German\n",
    "dataset_de = load_dataset(\"mlsum\", \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659a54b8-5ade-4e3a-abe8-847dd20f8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CNN/DailyMail dataset for english\n",
    "dataset_eng = load_dataset('cnn_dailymail', '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96ed3370-b80b-472b-9cec-522270f41054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET-FRENCH DETAILS: {'train': (392902, 6), 'validation': (16059, 6), 'test': (15828, 6)}\n",
      "DATASET-GERMAN DETAILS: {'train': (220887, 6), 'validation': (11394, 6), 'test': (10701, 6)}\n",
      "DATASET-ENGLISH DETAILS: {'train': (287113, 3), 'validation': (13368, 3), 'test': (11490, 3)}\n"
     ]
    }
   ],
   "source": [
    "# Print dataset details\n",
    "print(\"DATASET-FRENCH DETAILS:\",dataset_fr.shape)\n",
    "print(\"DATASET-GERMAN DETAILS:\", dataset_de.shape)\n",
    "print(\"DATASET-ENGLISH DETAILS:\",dataset_eng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029dc565-002e-4d24-b2b9-381c1d8a771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bifercating the datsets (splits)\n",
    "train_fr = dataset_fr['train']\n",
    "validation_fr = dataset_fr['validation']\n",
    "test_fr = dataset_fr['test']\n",
    "\n",
    "train_de = dataset_de['train']\n",
    "validation_de = dataset_de['validation']\n",
    "test_de = dataset_de['test']\n",
    "\n",
    "train_eng = dataset_eng['train']\n",
    "validation_eng = dataset_eng['validation']\n",
    "test_eng = dataset_eng['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b66bbe35-05e9-46e0-a701-a7b52d4e8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "train_df_fr = pd.DataFrame(train_fr)\n",
    "validation_df_fr = pd.DataFrame(validation_fr)\n",
    "test_df_fr = pd.DataFrame(test_fr)\n",
    "\n",
    "train_df_de = pd.DataFrame(train_de)\n",
    "validation_df_de = pd.DataFrame(validation_de)\n",
    "test_df_de = pd.DataFrame(test_de)\n",
    "\n",
    "train_df_eng = pd.DataFrame(train_eng)\n",
    "validation_df_eng = pd.DataFrame(validation_eng)\n",
    "test_df_eng = pd.DataFrame(test_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8111881-f75b-4254-8806-34b3e69cb15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
       "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
       "      <td>livres</td>\n",
       "      <td>https://www.lemonde.fr/livres/article/2010/01/...</td>\n",
       "      <td>La rentrée littéraire promet un programme de b...</td>\n",
       "      <td>01/01/2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Une semaine après l'attaque terroriste manquée...</td>\n",
       "      <td>Cette demande intervient une semaine après l'a...</td>\n",
       "      <td>proche-orient</td>\n",
       "      <td>https://www.lemonde.fr/proche-orient/article/2...</td>\n",
       "      <td>Gordon Brown appelle à une réunion internation...</td>\n",
       "      <td>01/01/2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Jean-Jacques Schuhl, Gilles Leroy, Christian G...   \n",
       "1  Une semaine après l'attaque terroriste manquée...   \n",
       "\n",
       "                                             summary          topic  \\\n",
       "0  Jean-Jacques Schuhl, Gilles Leroy, Christian G...         livres   \n",
       "1  Cette demande intervient une semaine après l'a...  proche-orient   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.lemonde.fr/livres/article/2010/01/...   \n",
       "1  https://www.lemonde.fr/proche-orient/article/2...   \n",
       "\n",
       "                                               title        date  \n",
       "0  La rentrée littéraire promet un programme de b...  01/01/2010  \n",
       "1  Gordon Brown appelle à une réunion internation...  01/01/2010  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_fr.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08f3e8ed-5fc4-4830-8773-230602922932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79051</th>\n",
       "      <td>Unter den Internetunternehmen ist Twitter eine...</td>\n",
       "      <td>Unter den Internetunternehmen ist Twitter eine...</td>\n",
       "      <td>wirtschaft</td>\n",
       "      <td>https://www.sueddeutsche.de/wirtschaft/twitter...</td>\n",
       "      <td>Twitter-Börsengang: Reich durch Zwitschern</td>\n",
       "      <td>00/10/2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71089</th>\n",
       "      <td>Nach einem Sabbatjahr kehrt Ronnie O'Sullivan ...</td>\n",
       "      <td>Nach einem Sabbatjahr kehrt Ronnie O'Sullivan ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>https://www.sueddeutsche.de/sport/snooker-welt...</td>\n",
       "      <td>Snooker-Weltmeister Ronnie O'Sullivan - Erholt...</td>\n",
       "      <td>00/05/2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "79051  Unter den Internetunternehmen ist Twitter eine...   \n",
       "71089  Nach einem Sabbatjahr kehrt Ronnie O'Sullivan ...   \n",
       "\n",
       "                                                 summary       topic  \\\n",
       "79051  Unter den Internetunternehmen ist Twitter eine...  wirtschaft   \n",
       "71089  Nach einem Sabbatjahr kehrt Ronnie O'Sullivan ...       sport   \n",
       "\n",
       "                                                     url  \\\n",
       "79051  https://www.sueddeutsche.de/wirtschaft/twitter...   \n",
       "71089  https://www.sueddeutsche.de/sport/snooker-welt...   \n",
       "\n",
       "                                                   title        date  \n",
       "79051         Twitter-Börsengang: Reich durch Zwitschern  00/10/2013  \n",
       "71089  Snooker-Weltmeister Ronnie O'Sullivan - Erholt...  00/05/2013  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_de.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e865750a-9a6c-4a9f-a7e1-f218995d0304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "      <td>42c027e4ff9730fbb3de84c1af0d2c506e41c3e4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "      <td>ee8871b15c50d0db17b0179a6d2beab35065f1e9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  LONDON, England (Reuters) -- Harry Potter star...   \n",
       "1  Editor's note: In our Behind the Scenes series...   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Harry Potter star Daniel Radcliffe gets £20M f...   \n",
       "1  Mentally ill inmates in Miami are housed on th...   \n",
       "\n",
       "                                         id  \n",
       "0  42c027e4ff9730fbb3de84c1af0d2c506e41c3e4  \n",
       "1  ee8871b15c50d0db17b0179a6d2beab35065f1e9  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_eng.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fae2df2-d439-4eb8-b356-ec8addd8cf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>After West Ham announced a vast reduction in s...</td>\n",
       "      <td>West Ham became first Premier League club to d...</td>\n",
       "      <td>fd984802497ba123291ca39b1f763d3ae195d831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6292</th>\n",
       "      <td>Hassan Munshi, one of two teenagers feared to ...</td>\n",
       "      <td>Families of Hassan Munshi and Talha Asmal, bot...</td>\n",
       "      <td>b410ef51a9d6c9b4566b2d69b02e877500f07357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  \\\n",
       "3788  After West Ham announced a vast reduction in s...   \n",
       "6292  Hassan Munshi, one of two teenagers feared to ...   \n",
       "\n",
       "                                             highlights  \\\n",
       "3788  West Ham became first Premier League club to d...   \n",
       "6292  Families of Hassan Munshi and Talha Asmal, bot...   \n",
       "\n",
       "                                            id  \n",
       "3788  fd984802497ba123291ca39b1f763d3ae195d831  \n",
       "6292  b410ef51a9d6c9b4566b2d69b02e877500f07357  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_eng.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b2870e-28b2-439c-b882-49c67a797421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.7 s, sys: 1.51 s, total: 10.2 s\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# only want the text/article and summary/highlights columns from the three datasets (French, German, English) for now\n",
    "# Convert to Pandas DataFrame and select only the required columns (French)\n",
    "train_df_fr1 = pd.DataFrame(train_fr)[['text', 'summary']]\n",
    "validation_df_fr1 = pd.DataFrame(validation_fr)[['text', 'summary']]\n",
    "test_df_fr1 = pd.DataFrame(test_fr)[['text', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec9a93d-5691-4f18-883f-154daa42555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 707 ms, total: 5.31 s\n",
      "Wall time: 8.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Convert to Pandas DataFrame and select only the required columns (German)\n",
    "train_df_de1 = pd.DataFrame(train_de)[['text', 'summary']]\n",
    "validation_df_de1 = pd.DataFrame(validation_de)[['text', 'summary']]\n",
    "test_df_de1 = pd.DataFrame(test_de)[['text', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb053032-d262-44ee-9e4c-ed92d2adb158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.62 s, sys: 1.19 s, total: 4.81 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Convert to Pandas DataFrame and select only the required columns (English)\n",
    "train_df_eng1 = pd.DataFrame(train_eng)[['article', 'highlights']]\n",
    "validation_df_eng1 = pd.DataFrame(validation_eng)[['article', 'highlights']]\n",
    "test_df_eng1 = pd.DataFrame(test_eng)[['article', 'highlights']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "940f3285-1ae6-4c8f-bfe4-b56869f98f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
       "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Une semaine après l'attaque terroriste manquée...</td>\n",
       "      <td>Cette demande intervient une semaine après l'a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Jean-Jacques Schuhl, Gilles Leroy, Christian G...   \n",
       "1  Une semaine après l'attaque terroriste manquée...   \n",
       "\n",
       "                                             summary  \n",
       "0  Jean-Jacques Schuhl, Gilles Leroy, Christian G...  \n",
       "1  Cette demande intervient une semaine après l'a...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the resulting DataFrames to verify the columns\n",
    "train_df_fr1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bc00b39-f25c-463f-8545-0660bc8e41bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220885</th>\n",
       "      <td>In Deutschland gibt es ihn bisher vor allem an...</td>\n",
       "      <td>Explosiv und höllisch stark: Das chinesische N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220886</th>\n",
       "      <td>Der Weihnachtsbaum vor dem Reichstag ist eine ...</td>\n",
       "      <td>Die Deutschen lieben ihre Weihnachtsbäume. Abe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "220885  In Deutschland gibt es ihn bisher vor allem an...   \n",
       "220886  Der Weihnachtsbaum vor dem Reichstag ist eine ...   \n",
       "\n",
       "                                                  summary  \n",
       "220885  Explosiv und höllisch stark: Das chinesische N...  \n",
       "220886  Die Deutschen lieben ihre Weihnachtsbäume. Abe...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_de1.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "903356a0-dcff-4fe2-83f0-0e0baa5000f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  LONDON, England (Reuters) -- Harry Potter star...   \n",
       "1  Editor's note: In our Behind the Scenes series...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Harry Potter star Daniel Radcliffe gets £20M f...  \n",
       "1  Mentally ill inmates in Miami are housed on th...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_eng1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d89ad-8d84-4a09-b249-3ba7354b9f71",
   "metadata": {},
   "source": [
    "## STEP 2: Data Cleaning: Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58711f6-259a-4f2c-9cea-d2a9e1e05c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Function to preprocess the dataset\n",
    "def preprocess_dataset(df, text_column, summary_column):\n",
    "    df[text_column] = df[text_column].apply(preprocess_text)\n",
    "    df[summary_column] = df[summary_column].apply(preprocess_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2276d828-30c0-430a-a549-534d9f0f2cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.5 s, sys: 5.85 s, total: 57.3 s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess the French dataset\n",
    "train_df_fr_processed = preprocess_dataset(train_df_fr1, 'text', 'summary')\n",
    "validation_df_fr_processed = preprocess_dataset(validation_df_fr1, 'text', 'summary')\n",
    "test_df_fr_processed = preprocess_dataset(test_df_fr1, 'text', 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67dd180e-8b87-4222-aab6-8dfc95110e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.3 s, sys: 1.84 s, total: 33.1 s\n",
      "Wall time: 38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess the German dataset\n",
    "train_df_de_processed = preprocess_dataset(train_df_de1, 'text', 'summary')\n",
    "validation_df_de_processed = preprocess_dataset(validation_df_de1, 'text', 'summary')\n",
    "test_df_de_processed = preprocess_dataset(test_df_de1, 'text', 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e019592-15da-4fdd-a8c3-65e34c3faeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.8 s, sys: 3.77 s, total: 34.5 s\n",
      "Wall time: 53.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess the English dataset\n",
    "train_df_eng_processed = preprocess_dataset(train_df_eng1, 'article', 'highlights')\n",
    "validation_df_eng_processed = preprocess_dataset(validation_df_eng1, 'article', 'highlights')\n",
    "test_df_eng_processed = preprocess_dataset(test_df_eng1, 'article', 'highlights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c3b385d-125a-46a9-8d8f-de6aecfaf54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jeanjacques schuhl gilles leroy christian gail...</td>\n",
       "      <td>jeanjacques schuhl gilles leroy christian gail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>une semaine après lattaque terroriste manquée ...</td>\n",
       "      <td>cette demande intervient une semaine après lat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  jeanjacques schuhl gilles leroy christian gail...   \n",
       "1  une semaine après lattaque terroriste manquée ...   \n",
       "\n",
       "                                             summary  \n",
       "0  jeanjacques schuhl gilles leroy christian gail...  \n",
       "1  cette demande intervient une semaine après lat...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the preprocessed French DataFrame\n",
    "train_df_fr_processed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71f5d71b-de1e-4f26-ad47-f7c6eea4f297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220885</th>\n",
       "      <td>in deutschland gibt es ihn bisher vor allem an...</td>\n",
       "      <td>explosiv und höllisch stark das chinesische na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220886</th>\n",
       "      <td>der weihnachtsbaum vor dem reichstag ist eine ...</td>\n",
       "      <td>die deutschen lieben ihre weihnachtsbäume aber...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "220885  in deutschland gibt es ihn bisher vor allem an...   \n",
       "220886  der weihnachtsbaum vor dem reichstag ist eine ...   \n",
       "\n",
       "                                                  summary  \n",
       "220885  explosiv und höllisch stark das chinesische na...  \n",
       "220886  die deutschen lieben ihre weihnachtsbäume aber...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the preprocessed German DataFrame\n",
    "train_df_de_processed.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15d755d9-fb58-4983-90f0-c27a0e73263e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>london england reuters  harry potter star dani...</td>\n",
       "      <td>harry potter star daniel radcliffe gets £20m f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>editors note in our behind the scenes series c...</td>\n",
       "      <td>mentally ill inmates in miami are housed on th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  london england reuters  harry potter star dani...   \n",
       "1  editors note in our behind the scenes series c...   \n",
       "\n",
       "                                          highlights  \n",
       "0  harry potter star daniel radcliffe gets £20m f...  \n",
       "1  mentally ill inmates in miami are housed on th...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the preprocessed English DataFrame\n",
    "train_df_eng_processed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "489a26bc-dbc3-44a0-91db-26a490e976e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of each, we will rename the English dataset's columns to the ones of the other two as follows:\n",
    "# Rename the columns in the English dataset\n",
    "train_df_eng_processed.rename(columns={'article': 'text', 'highlights': 'summary'}, inplace=True)\n",
    "validation_df_eng_processed.rename(columns={'article': 'text', 'highlights': 'summary'}, inplace=True)\n",
    "test_df_eng_processed.rename(columns={'article': 'text', 'highlights': 'summary'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd764225-69f5-4dfa-ad24-8b153d313d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>london england reuters  harry potter star dani...</td>\n",
       "      <td>harry potter star daniel radcliffe gets £20m f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>editors note in our behind the scenes series c...</td>\n",
       "      <td>mentally ill inmates in miami are housed on th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  london england reuters  harry potter star dani...   \n",
       "1  editors note in our behind the scenes series c...   \n",
       "\n",
       "                                             summary  \n",
       "0  harry potter star daniel radcliffe gets £20m f...  \n",
       "1  mentally ill inmates in miami are housed on th...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the resulting DataFrames to verify the column names\n",
    "train_df_eng_processed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74913b65-2257-40d4-903e-dcdeef0102e8",
   "metadata": {},
   "source": [
    "## STEP 3: Data size reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b06eb-57a0-48c4-bb3c-79fbb5693ab5",
   "metadata": {},
   "source": [
    "Reducing the size of our training dataset is a crucial step to ensure the efficient use of computational resources and prevent potential crashes during the training process. Given the large size of our datasets—such as the French dataset with nearly `400,000` observations—it is important to balance the need for a representative sample with the limitations of our computing environment. \n",
    "\n",
    "By randomly sampling a subset of `100,000` observations, we maintain the diversity and representativeness of the data while significantly decreasing the computational load. This reduction allows us to streamline the training process, making it more manageable and ensuring smoother execution within the constraints of our Jupyter Notebook environment. This approach is particularly useful at this stage, as it facilitates faster iterations and debugging, ultimately leading to more efficient model development and refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "957caa60-6fb6-4639-9e53-a18b2b3879c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 100,000 observations from each training dataset\n",
    "\n",
    "# French dataset\n",
    "train_df_fr_sampled = train_df_fr_processed.sample(n=100000, random_state=42)\n",
    "\n",
    "# German dataset\n",
    "train_df_de_sampled = train_df_de_processed.sample(n=100000, random_state=42)\n",
    "\n",
    "# English dataset\n",
    "train_df_eng_sampled = train_df_eng_processed.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6586763a-f60c-46b4-a964-86c56d367d84",
   "metadata": {},
   "source": [
    "## STEP 4: Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfaada4c-e957-498e-9a1d-70df1ad98a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of train_df_fr_processed in a new variable train_df_tokenised for all three datasets\n",
    "train_df_fr_tokenised = train_df_fr_sampled.copy()\n",
    "validation_df_fr_tokenised = validation_df_fr_processed.copy()\n",
    "test_df_fr_tokenised = test_df_fr_processed.copy()\n",
    "\n",
    "train_df_de_tokenised = train_df_de_sampled.copy()\n",
    "validation_df_de_tokenised = validation_df_de_processed.copy()\n",
    "test_df_de_tokenised = test_df_de_processed.copy()\n",
    "\n",
    "train_df_eng_tokenised = train_df_eng_sampled.copy()\n",
    "validation_df_eng_tokenised = validation_df_eng_processed.copy()\n",
    "test_df_eng_tokenised = test_df_eng_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d15f19-a930-4488-be91-2ef7b71d7c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify by printing the first few rows of the new DataFrame\n",
    "train_df_fr_tokenised.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d865cf3-a38b-4545-b463-fe216257b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer\n",
    "tokenizer = MBart50Tokenizer.from_pretrained('facebook/mbart-large-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f229f80-6c16-4ef0-9f5e-cd87024f50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and prepare text using MBart50Tokenizer\n",
    "def tokenize_and_prepare(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", max_length=512, padding='max_length', truncation=True)\n",
    "    return tokens.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1dc5271-e90d-492a-974c-99e587551d04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df_fr_tokenised' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply tokenization to the French dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_df_fr_tokenised\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df_fr_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(tokenize_and_prepare)\n\u001b[1;32m      3\u001b[0m train_df_fr_tokenised[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df_fr_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(tokenize_and_prepare)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df_fr_tokenised' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to the French dataset\n",
    "train_df_fr_tokenised['text'] = train_df_fr_processed['text'].apply(tokenize_and_prepare)\n",
    "train_df_fr_tokenised['summary'] = train_df_fr_processed['summary'].apply(tokenize_and_prepare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d038b8-6b4c-4557-9ec7-5de793d56d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df_fr_tokenised['text'] = validation_df_fr_processed['text'].apply(tokenize_and_prepare)\n",
    "validation_df_fr_tokenised['summary'] = validation_df_fr_processed['summary'].apply(tokenize_and_prepare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a1548-f6f5-4f5b-9996-3ccb544fede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_fr_processed['text'] = test_df_fr_processed['text'].apply(tokenize_and_prepare)\n",
    "test_df_fr_processed['summary'] = test_df_fr_processed['summary'].apply(tokenize_and_prepare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e1b53-b3cc-499c-9f63-4e77bcf75d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the tokenized French DataFrame\n",
    "train_df_fr_tokenised.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb5835-4daf-42e8-a6d7-17a78b969738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49670c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['text'], max_length=512, padding='max_length', truncation=True)\n",
    "    targets = tokenizer(examples['summary'], max_length=150, padding='max_length', truncation=True)\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': targets['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43573d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51b69dde6724ae5922cdc44245f5ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/392902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2003250ae4f64170b89b1969f1ab09d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16059 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1569f3867ed2424bb4ec0aad106ad9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying the preprocessing to the dataset\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fe4aec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3dfa5a0db74168b4d0d363364e2df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/392902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bce89da5404fc594b0642b9ebecde6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/16059 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd46e63c593b4e57b273ed1c6c826e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Optional: Save the processed dataset\n",
    "dataset.save_to_disk(\"processed_mlsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146b0fb",
   "metadata": {},
   "source": [
    "# __4. Model Selection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1e3f576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c797366fc2f40c9af1c936da10c5640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae2b5f425334066a0e7df48c7abf188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "\n",
    "# Load MBart model and tokenizer\n",
    "model_name = 'facebook/mbart-large-50'\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74ecb6",
   "metadata": {},
   "source": [
    "## __STEP 2: Training the Summarizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a84aa57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # Use 1 epoch for quick testing; adjust as needed\n",
    "    per_device_train_batch_size=2,  # Adjust based on your hardware capabilities\n",
    "    per_device_eval_batch_size=2,  # Adjust based on your hardware capabilities\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53c23949",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13aec938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafay/Documents/Anaconda/anaconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3b88e879f24a1db3a8a9d244c6bfa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/392902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4fa45ac0e94d659bc364e5476acbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16059 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb359ce86b546e39a5de7ad4b3f2c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Load MLSUM dataset for French\n",
    "dataset = load_dataset(\"mlsum\", \"fr\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-50')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['text'], max_length=512, padding='max_length', truncation=True)\n",
    "    targets = tokenizer(examples['summary'], max_length=150, padding='max_length', truncation=True)\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': targets['input_ids']\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69fc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='196451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     4/196451 1:08:45 < 112559:51:46, 0.00 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # Use 1 epoch for quick testing; adjust as needed\n",
    "    per_device_train_batch_size=2,  # Adjust based on your hardware capabilities\n",
    "    per_device_eval_batch_size=2,  # Adjust based on your hardware capabilities\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"mbart-summarizer\")\n",
    "\n",
    "# Function to generate summary\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "example_text = \"Votre texte en français ici pour résumer.\"\n",
    "summary = generate_summary(example_text)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b04617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
